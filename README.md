# NewsNexus v2.0 - Intelligent News Aggregator MCP Server

A production-ready, intelligent news aggregation system with **sophisticated topic filtering, multi-feed architecture, and MCP server support**. Designed for AI assistants and power users to fetch only relevant, recent news with zero false positives.

[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)
[![MCP Protocol](https://img.shields.io/badge/MCP-2024--11--05-green.svg)](https://modelcontextprotocol.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Production](https://img.shields.io/badge/Status-Production-brightgreen.svg)]()

## üéØ Overview

NewsNexus is a sophisticated news aggregator that fetches **only the latest news** (15 days by default) with intelligent topic filtering, priority-based fallback architecture, and parallel multi-feed processing. Built as a **Model Context Protocol (MCP) server** for seamless AI integration.

**Latest Update (Dec 10, 2025):** Fixed Google News RSS parsing bug, updated all 42 Google News URLs to US locale, and enhanced topic keywords to 17 categories with 500+ keywords.

### üåü Key Features

- ‚úÖ **Smart Recent News Filter**: Default 15-day cap ensures only recent/latest/top news (max 365 days)
- ‚úÖ **Intelligent Topic Filtering**: Word boundary matching + 17 topic categories with 500+ keywords
- ‚úÖ **3-Priority Fallback Architecture**: Official RSS ‚Üí Google News ‚Üí HTML Scraper
- ‚úÖ **Multi-Feed Parallel Processing**: Fetch 45+ RSS feeds simultaneously across priority domains
- ‚úÖ **Priority-Based Fetching**: Always respects domain priority order (NDTV > Indian Express > etc.)
- ‚úÖ **Flexible Domain Matching**: Use partial names (`openai` ‚Üí `openai.com`, `ndtv` ‚Üí `ndtv.com`)
- ‚úÖ **Smart Deduplication**: URL and title-based deduplication with fuzzy matching
- ‚úÖ **Reverse Chronological Order**: Articles always newest first
- ‚úÖ **Fast Performance**: ~2-3s per domain with parallel fetching and smart timeout management
- ‚úÖ **Production-Ready**: 8-layer filtering engine, comprehensive error handling, structured logging

---

## üìå Recent Fixes & Updates (December 2025)

### ‚ú® December 10, 2025 - Google News Fix & Topic Keyword Expansion

**Problems Fixed:**
1. **Google News RSS articles all deduplicated to 1** - `source.href` only contained domain, not full article URL
2. **Topic filter too strict** - Only matched exact keyword, missed related terms

**Solutions Implemented:**
```python
# Fix 1: Keep unique Google redirect URLs instead of extracting domain
# Google redirect URLs like news.google.com/rss/articles/... are unique and work when clicked

# Fix 2: Enhanced topic keywords with 17 categories
TOPIC_KEYWORDS = {
    'ai': ['ai', 'artificial intelligence', 'chatgpt', 'openai', 'gemini', 'claude', ...],  # 56 keywords
    'tech': ['technology', 'software', 'startup', 'gadget', 'smartphone', ...],  # 60 keywords
    'crypto': ['bitcoin', 'ethereum', 'blockchain', 'nft', 'defi', ...],  # 25 keywords
    'startup': ['unicorn', 'funding', 'venture capital', 'founder', ...],  # 28 keywords
    'gaming': ['esports', 'playstation', 'xbox', 'pubg', 'fortnite', ...],  # 26 keywords
    # + 12 more categories: cricket, finance, sports, politics, health, entertainment, 
    #                        education, auto, travel, weather, realestate, jobs
}
```

**Additional Changes:**
- Updated all 42 Google News URLs to US locale (`&hl=en-US&gl=US&ceid=US:en`) for better results
- Fixed TechCrunch RSS URL (was pointing to fake feedburner feed)
- Synced TOPIC_KEYWORDS between `main.py` and `fetch_topic_news.py`

### ‚ú® December 9, 2025 - Critical Topic Filtering Fix

**Problem Identified:**
- Substring matching was causing false positives: `"ai" in "paint"` ‚Üí `True`, `"ai" in "Ukraine"` ‚Üí `True`
- Priority domain AI fetches returned only 4 articles instead of expected 8+
- Articles about generic topics (sports, politics) were incorrectly labeled as AI-related

**Root Cause:**
- `filter_articles()` in `main.py` used simple substring matching: `if topic_lower not in text`
- `is_ai_related()` in `get_ai_news_with_fallback.py` used substring matching instead of word boundaries

**Solution Implemented:**
```python
# Before (broken):
if topic_lower and topic_lower not in text:
    continue

# After (fixed):
if topic_lower:
    if not re.search(r'\b' + re.escape(topic_lower) + r'\b', text):
        continue
```

**Impact:**
- ‚úÖ Priority domain AI fetches now return **8 articles from premium Indian sources only**
- ‚úÖ Zero false positives: "Ukraine" no longer matches "AI" filter
- ‚úÖ Correct fallback behavior: Only uses non-priority sources when priority sources insufficient
- ‚úÖ Proper keyword matching: "ChatGPT", "Machine Learning", "Anthropic Claude" all correctly identified

**Files Modified:**
1. `main.py` - Line 1367: Updated `filter_articles()` function with word boundary matching
2. `get_ai_news_with_fallback.py` - Line 67: Updated `is_ai_related()` function with word boundary matching
3. `fetch_ai_news.py` - Created wrapper to suppress logging for clean JSON output

**Testing Results:**
```
BEFORE FIX:
- ndtv.com: 0 AI articles
- indianexpress.com: 1 AI article
- timesofindia.indiatimes.com: 0 AI articles
- hindustantimes.com: 0 AI articles
- gadgets360.com: 0 AI articles
- economictimes.indiatimes.com: 3 AI articles
Total: 4 from priority, then 4 fallback to non-priority (Wired, Verge, TechCrunch)

AFTER FIX:
- ndtv.com: "The Year AI Became Personal: 10 Tools..."
- timesofindia.indiatimes.com: "Google's AI hub Tarluvada..."
- economictimes.indiatimes.com: "SoftBank, Nvidia looking to invest in Skild AI..."
- economictimes.indiatimes.com: "Intel signs pact for AI PC solutions..."
- economictimes.indiatimes.com: "Trump signs executive order on AI..."
- economictimes.indiatimes.com: "IBM buys Confluent for AI-driven demand..."
- ndtv.com: "OpenAI Claims Increased Enterprise Usage..."
- economictimes.indiatimes.com: "Sebi eases AIF rules, allows AI-only schemes..."
Total: 8 from priority domains only ‚úÖ
```

### üßπ December 9, 2025 - Production Code Cleanup

**Removed:**
- 14 test Python scripts (test_*.py, compare_sources.py, diagnose_articles.py, etc.)
- 19 documentation markdown files (temporary implementation notes)
- 4 output text files (ai_output.txt, comparison.txt, etc.)
- 3 temporary JSON files (ai_news_*.json)
- Python cache directories (__pycache__, .pytest_cache)
- Test artifacts (.coverage, tech_test.txt)

**Result:** Clean production structure with only 4 core Python files

---

## üèóÔ∏è Architecture



### System Flow & Architecture

```mermaid
graph TD
    %% Clients
    HttpClients[HTTP Clients<br/>(N8N, Postman, Web Apps)]
    StdioClients[STDIO Clients<br/>(Claude Desktop Local, Scripts)]

    %% Transport Layer
    subgraph "Transport Layer (Interfaces)"
        HttpServer[HTTP Server<br/>(FastAPI / http_server.py)]
        StdioServer[STDIO Server<br/>(stdin/stdout loop / main.py)]
    end

    %% Core Logic
    subgraph "Core Backend Logic (Shared)"
        Dispatcher[handle_request()<br/>(The 'Brain' in main.py)]
        
        Tools[Tool Functions]
        GetArticles[get_articles()]
        GetTopNews[get_top_news()]
        Health[health_check()]
    end

    %% Fetching Engine
    subgraph "News Engine"
        Fetcher[Parallel Fetcher]
        RSS[RSS Feeds]
        Scraper[HTML Scraper]
        GNews[Google News]
    end

    %% The Flow - HTTP Path
    HttpClients -->|1. HTTP POST /mcp| HttpServer
    HttpServer -->|2. Extract JSON| Dispatcher

    %% The Flow - STDIO Path
    StdioClients -->|1. Write JSON to stdin| StdioServer
    StdioServer -->|2. Parse JSON| Dispatcher

    %% The Shared Execution Flow
    Dispatcher -->|3. Route to Tool| Tools
    Tools -->|4. Execute Logic| Fetcher
    Fetcher --> RSS
    Fetcher --> Scraper
    Fetcher --> GNews

    %% The Return Path
    Fetcher -->|5. Raw Articles| Tools
    Tools -->|6. JSON Result| Dispatcher
    
    Dispatcher -->|7a. Return Object| HttpServer
    HttpServer -->|8a. HTTP 200 OK| HttpClients

    Dispatcher -->|7b. Return Object| StdioServer
    StdioServer -->|8b. Print to stdout| StdioClients

    %% Styling
    classDef client fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef interface fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;
    classDef core fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;
    classDef engine fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px;

    class HttpClients,StdioClients client;
    class HttpServer,StdioServer interface;
    class Dispatcher,Tools,GetArticles,GetTopNews,Health core;
    class Fetcher,RSS,Scraper,GNews engine;
```

## üìã Fetching & Filtering Rules

### üîπ Rule 1: 15-Day Default Cap for "Recent" News
**When user says**: "recent", "top", "latest" news/articles (without specifying days)

**Behavior**: 
- Automatically fetch only from **last 15 days**
- Hard cap: Cannot exceed 15 days even if user requests more (unless explicit `lastNDays` parameter)
- Default article count: **10 articles**

**Examples**:
```
User: "Get recent AI news"          ‚Üí Last 15 days, 10 articles
User: "Top news from TechCrunch"    ‚Üí Last 15 days, 10 articles
User: "Latest articles about India" ‚Üí Last 15 days, 10 articles
```

### üîπ Rule 2: Reverse Chronological Order (Newest First)
**Guarantee**: Latest article is **always** fetched first

**Behavior**:
- All articles sorted by `published_at` in descending order
- Most recent article = index 0
- Oldest article = last index

**Example**:
```json
{
  "articles": [
    {"title": "Breaking: AI Breakthrough", "published_at": "2024-12-08T10:00:00Z"},  // ‚Üê Newest
    {"title": "Tech Update Yesterday", "published_at": "2024-12-07T15:30:00Z"},
    {"title": "News from 3 days ago", "published_at": "2024-12-05T09:00:00Z"}       // ‚Üê Oldest
  ]
}
```

### üîπ Rule 3: Priority-Based Source Fallback
**When to check next priority**: Only if required number of articles **not** fetched from current priority

**Threshold**: Minimum **5 articles** before considering current priority successful

**Behavior**:
1. Try Priority 1 (Official RSS feeds) - fetch all feeds in parallel
2. If < 5 articles ‚Üí Try Priority 2 (Google News RSS)
3. If < 5 articles OR Google News quality fails ‚Üí Try Priority 3 (Scraper)
4. Always keep best fallback if no priority meets threshold

**Example Flow**:
```
Priority 1 (RSS): 3 articles found    ‚Üí < 5, try next priority
Priority 2 (Google): 8 articles found ‚Üí ‚â• 5, SUCCESS! Return these 8 articles
```

### üîπ Rule 4: No Data > 15 Days by Default
**Default behavior**: Never fetch articles older than 15 days

**Exceptions**: 
- User explicitly sets `lastNDays` parameter (e.g., `lastNDays=30`)
- User provides complete article URL directly

**Date filtering**:
- Published date parsed from RSS/HTML
- Age calculated from current UTC time
- Articles > 15 days automatically excluded (unless user overrides)

### üîπ Rule 5: Comprehensive Filtering (8 Criteria)

| Filter Type | How It Works | Example |
|-------------|--------------|---------|
| **Days** | Reject articles older than `lastNDays` | `lastNDays=7` ‚Üí Only last 7 days |
| **Topic** | Keyword search in title/summary/tags | `topic="AI"` ‚Üí Match "AI", "artificial intelligence" |
| **Location** | Keyword search in article text | `location="India"` ‚Üí Match "India", "Indian", "Mumbai" |
| **Site Priority** | Fetch from top-rated sources first | Priority 1-6 sites for `get_top_news()` |
| **URL Dedup** | Track URLs in `seen_urls` set | Normalize & compare URLs |
| **Title Dedup** | Fuzzy title matching | "AI Breakthrough" vs "AI BREAKTHROUGH!" ‚Üí duplicate |
| **Response Time** | 2s timeout per source, 10s per priority group | Slow sources skipped |
| **Article Count** | Limit to `count` parameter | `count=5` ‚Üí Return max 5 articles |

### üîπ Rule 6: Parallel & Random Fetching
**Parallelization**: All sources at **same priority level** fetched simultaneously

**Implementation**:
- `ThreadPoolExecutor` with max **8 workers**
- Timeout: **10 seconds** per priority group
- Deduplication: Remove URL & title duplicates across all parallel results

**Example** (Priority 1 with 5 RSS feeds):
```
Thread 1: Fetch RSS Feed 1 ‚Üí 10 articles
Thread 2: Fetch RSS Feed 2 ‚Üí 8 articles   } All fetched in parallel
Thread 3: Fetch RSS Feed 3 ‚Üí 12 articles  } (~1-2 seconds total)
Thread 4: Fetch RSS Feed 4 ‚Üí 5 articles
Thread 5: Fetch RSS Feed 5 ‚Üí 9 articles

Combined: 44 articles ‚Üí Deduplicate ‚Üí 26 unique articles
```

### üîπ Rule 7: Google News Quality Controls
**Problem**: Google News RSS often returns redirect URLs (e.g., `news.google.com/rss/articles/...`)

**Solution**: Quality validation & fallback

**Quality Check Process**:
1. Sample first 5 articles from Google News results
2. Attempt to resolve redirect URLs (2s timeout each)
3. Count how many have valid, direct URLs (not `news.google.com`)
4. If < 50% valid ‚Üí Treat as failure, fallback to Priority 3 (Scraper)
5. If ‚â• 50% valid ‚Üí Use Google News results

**Additional Checks**:
- No articles older than 15 days (default)
- Redirect resolution with timeout
- Automatic fallback if quality fails

### üîπ Rule 8: Explicit-Only Sites with Partial Matching
**Two types of sites**:
1. **Priority sites** (6 configured): Used for `get_top_news()`
2. **Explicit-only sites** (39 configured): Accessed only when user specifies domain

**Flexible domain matching** for explicit sites:
- Exact match: `openai.com` ‚Üí `openai.com` ‚úÖ
- Partial match: `openai` ‚Üí `openai.com` ‚úÖ
- www prefix: `www.openai.com` ‚Üí `openai.com` ‚úÖ

**Examples**:
```
User: "News from openai"      ‚Üí Matches openai.com
User: "Top articles from wired" ‚Üí Matches wired.com
User: "Get techcrunch news"   ‚Üí Matches techcrunch.com
```

### üîπ Rule 9: Priority Sites for Top News
**get_top_news() behavior**: Fetch from **6 priority sites** (expandable to 12)

**Current Priority Sites**:
1. ndtv.com (priority 1)
2. indianexpress.com (priority 2)
3. timesofindia.indiatimes.com (priority 3)
4. hindustantimes.com (priority 4)
5. gadgets360.com (priority 5)
6. economictimes.indiatimes.com (priority 6)

**Process**:
- Fetch from each priority site in parallel
- Apply topic/location/date filters
- Combine & deduplicate results
- Sort newest first
- Return top N articles (default: 10)

**Example**:
```
User: "Get top 10 news"
‚Üí Fetch from all 6 priority sites in parallel
‚Üí Combine ~30-50 articles
‚Üí Filter, deduplicate, sort newest first
‚Üí Return top 10 articles
```


## üöÄ Quick Start

### Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

---

## üåê Transport Modes: STDIO vs HTTP vs SSE

NewsNexus supports **3 transport modes** for different use cases:

| Mode | URL/Command | Best For | Runs As |
|------|-------------|----------|---------|
| **STDIO** | `python main.py` | Local MCP clients (Claude Desktop local) | Subprocess |
| **HTTP** | `http://localhost:8000/mcp` | External apps, N8N, APIs | Background server |
| **SSE** | `http://localhost:8000/sse` | Real-time streaming clients | Background server |

### üìå Key Differences

| Feature | STDIO | HTTP | SSE |
|---------|-------|------|-----|
| **How it works** | Reads JSON from stdin, writes to stdout | POST JSON, get JSON response | Long-lived connection, server pushes events |
| **Connection** | Subprocess per session | Stateless requests | Persistent connection |
| **Server lifecycle** | Started by MCP client | Must be running before use | Must be running before use |
| **Use case** | Local AI assistants | External apps, webhooks, automation | Real-time updates |

### ‚ö†Ô∏è Important: HTTP/SSE Requires Running Server

When using HTTP or SSE:
1. You must **start the HTTP server first**: `python http_server.py`
2. The server runs on port 8000 by default
3. Keep the server running while external apps make requests

For STDIO (default), the MCP client starts/stops the server automatically.

---

## üîå Integration Guides

### Claude Desktop Integration

**Option 1: STDIO (Recommended for local use)**

Add to `claude_desktop_config.json`:
```json
{
  "mcpServers": {
    "news-nexus": {
      "command": "python",
      "args": ["C:\\path\\to\\NewsNexus\\main.py"]
    }
  }
}
```
Claude starts the server automatically. No manual server management needed.

**Option 2: HTTP (For remote/shared access)**

1. Start the HTTP server (keep it running):
   ```bash
   python http_server.py --port 8000
   ```

2. Configure Claude Desktop to use HTTP endpoint:
   ```json
   {
     "mcpServers": {
       "news-nexus": {
         "url": "http://localhost:8000/sse"
       }
     }
   }
   ```

---

### N8N Integration (HTTP)

N8N uses HTTP requests, so you need the HTTP server running.

**Step 1: Start the HTTP Server**
```bash
cd C:\path\to\NewsNexus
python http_server.py
# Server runs on http://localhost:8000
```

**Step 2: Use HTTP Request Node in N8N**

| Setting | Value |
|---------|-------|
| Method | POST |
| URL | `http://localhost:8000/mcp` |
| Content-Type | `application/json` |

**Example: Get Top AI News**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "get_top_news",
    "arguments": {
      "count": 10,
      "topic": "ai"
    }
  }
}
```

**Example: Get Articles from TechCrunch**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "get_articles",
    "arguments": {
      "domain": "techcrunch.com",
      "topic": "AI",
      "count": 5
    }
  }
}
```

**Simpler REST API (No MCP JSON-RPC needed)**

N8N can also use the simpler REST endpoints:

| Endpoint | Method | Example |
|----------|--------|---------|
| `/api/articles` | GET | `http://localhost:8000/api/articles?domain=techcrunch.com&count=5&topic=ai` |
| `/api/top-news` | GET | `http://localhost:8000/api/top-news?count=10&topic=ai&location=india` |
| `/api/health` | GET | `http://localhost:8000/api/health` |

---

### cURL Examples (HTTP)

```bash
# Start server first
python http_server.py

# List available tools
curl -X POST http://localhost:8000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'

# Get top 10 AI news
curl -X POST http://localhost:8000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","id":1,"method":"tools/call","params":{"name":"get_top_news","arguments":{"count":10,"topic":"ai"}}}'

# Simple REST API
curl "http://localhost:8000/api/top-news?count=10&topic=ai"
curl "http://localhost:8000/api/articles?domain=techcrunch.com&count=5"
```

---

### Running HTTP Server as Background Service (Windows)

For production use, run the HTTP server as a background service:

```powershell
# Start as background job
Start-Job -ScriptBlock { python C:\path\to\NewsNexus\http_server.py }

# Or use nssm to create a Windows service
nssm install NewsNexus python http_server.py
nssm start NewsNexus
```

---

## ‚úÖ HTTP API Covers All Features

The HTTP wrapper provides **full access to all NewsNexus functionality**:

| Feature | MCP Tool | HTTP REST API |
|---------|----------|---------------|
| Top/Recent/Latest news | `get_top_news` | `GET /api/top-news` |
| Articles from domain | `get_articles` | `GET /api/articles?domain=...` |
| Filter by topic | `topic` param | `?topic=ai` |
| Filter by location | `location` param | `?location=india` |
| Filter by date | `lastNDays` param | `?lastNDays=7` |
| Article count | `count` param | `?count=10` |
| Health check | `health_check` | `GET /api/health` |
| Metrics | `get_metrics` | `GET /api/metrics` |

**All filters work the same way in HTTP as STDIO!**

---

### MCP Server Setup (STDIO - For AI Assistants)

Add to your MCP configuration (e.g., `%APPDATA%\Code\User\globalStorage\saoudrizwan.claude-dev\settings\cline_mcp_settings.json`):

```json
{
  "mcpServers": {
    "news-nexus": {
      "command": "python",
      "args": ["C:\\Swdtools\\NewsNexus\\main.py"]
    }
  }
}
```

### Usage Examples

#### Get Articles from Specific Domain
```python
# Partial domain matching
get_articles(domain="openai", count=5)  # Matches openai.com
get_articles(domain="wired", count=10)  # Matches wired.com

# With filters
get_articles(
    domain="techcrunch",
    topic="AI",           # Filter by topic
    location="India",     # Filter by location
    lastNDays=7,          # Last 7 days only
    count=15              # Return 15 articles
)
```

#### Get Top News from Priority Sites
```python
# Default: 10 articles from last 15 days
get_top_news()

# With filters
get_top_news(
    count=20,            # Return 20 articles
    topic="technology",   # Filter by topic
    location="India",     # Filter by location
    lastNDays=3          # Last 3 days only
)
```

#### Health Check
```python
health_check()  # Returns server status, domain count, cache info
```

#### Get Metrics
```python
get_metrics()  # Returns performance metrics, counters, histograms
```

## üìä Sites Configuration

### Total Domains: 45

#### Priority Sites (6) - Used by `get_top_news()`
1. ndtv.com (priority 1) - 5 RSS feeds
2. indianexpress.com (priority 2) - 5 RSS feeds
3. timesofindia.indiatimes.com (priority 3) - 5 RSS feeds
4. hindustantimes.com (priority 4) - 2 RSS feeds
5. gadgets360.com (priority 5) - 2 RSS feeds
6. economictimes.indiatimes.com (priority 6) - 5 RSS feeds

**Total RSS feeds**: 24 across 6 priority sites

#### Explicit-Only Sites (39)
Accessed only when user specifies domain name (supports partial matching):

**Technology & AI**: analyticsindiamag.com, indiatechnologynews.in, wired.com, theverge.com, techradar.com, techrepublic.com, devshorts.in, venturebeat.com, xda-developers.com, cnet.com, gizmodo.com, engadget.com, zdnet.com

**AI Research**: research.google, analyticsvidhya.com, kdnuggets.com, infoworld.com, artificialintelligence-news.com, techbuzz.ai, openai.com, deepmind.google, huggingface.co

**Developer**: github.blog, hackernoon.com

**General News**: bbc.co.uk, news.google.com, theguardian.com, sportskeeda.com

**How-To & Lifestyle**: howtogeek.com, wonderhowto.com, lifehacker.com, creatoreconomy.so

**Tech News**: techcrunch.com, mashable.com, androidpolice.com, aitechin.substack.com, business-standard.com

**Community**: news.ycombinator.com (Hacker News)

### Multi-Feed Sites (7 sites with 26 total RSS feeds)
These sites use **multiple RSS feeds fetched in parallel** for comprehensive coverage:

| Site | RSS Feeds | Example Feeds |
|------|-----------|---------------|
| ndtv.com | 5 | Top Stories, Latest News, Trending, Gadgets, Sports |
| indianexpress.com | 5 | Cricket, Trending, Technology, Tech News, AI |
| timesofindia.indiatimes.com | 5 | Top Stories, Most Recent, India, Cities, Tech |
| hindustantimes.com | 2 | India News, Technology |
| gadgets360.com | 2 | All Feeds, News |
| economictimes.indiatimes.com | 5 | Default, Markets, Market Data, Tech, AI |
| venturebeat.com | 2 | Main Feed, AI Category |


## üìñ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `NEWSNEXUS_LOG_LEVEL` | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR) |
| `NEWSNEXUS_MAX_ARTICLES` | `50` | Maximum articles per request |
| `NEWSNEXUS_CACHE_TTL` | `300` | Cache TTL in seconds (5 minutes) |
| `NEWSNEXUS_RATE_LIMIT` | `10` | Max requests per window per domain |
| `NEWSNEXUS_RATE_WINDOW` | `60` | Rate limit window in seconds |
| `NEWSNEXUS_PARALLEL` | `true` | Enable parallel source fetching |
| `NEWSNEXUS_CONFIG_PATH` | `./sites.json` | Path to sites configuration |
| `NEWSNEXUS_DEEP_SCRAPE` | `true` | Enable deep HTML scraping |
| `NEWSNEXUS_DEEP_SCRAPE_MAX` | `10` | Max articles for deep scraping |
| `NEWSNEXUS_DEEP_WORKERS` | `5` | Parallel workers for scraping |

### Core Constants (Hardcoded in main.py)

| Constant | Value | Description |
|----------|-------|-------------|
| `MAX_RECENT_DAYS` | `15` | Hard cap for "recent" news (days) |
| `DEFAULT_ARTICLE_COUNT` | `10` | Default articles when user doesn't specify |
| `MIN_ARTICLES_THRESHOLD` | `5` | Minimum articles before trying next priority |
| `DEFAULT_TIMEOUT_MS` | `2000` | Default timeout per source (milliseconds) |
| `TOP_NEWS_SITE_LIMIT` | `12` | Max priority sites for get_top_news() |

### Site Configuration (`sites.json`)

Each site has multiple sources with priority levels:

```json
{
  "name": "NDTV Multi-Feed",
  "domain": "ndtv.com",
  "priority": 1,
  "sources": [
    {"type": "official_rss", "url": "https://feeds.feedburner.com/ndtvnews-top-stories", "priority": 1, "timeout_ms": 2000},
    {"type": "official_rss", "url": "https://feeds.feedburner.com/ndtvnews-latest", "priority": 1, "timeout_ms": 2000},
    {"type": "google_news", "url": "https://news.google.com/rss/search?q=site:ndtv.com&hl=en", "priority": 2, "timeout_ms": 2000},
    {"type": "scraper", "url": "https://www.ndtv.com/", "priority": 3, "timeout_ms": 2000}
  ]
}
```

**Source Types**:
- `official_rss`: Direct RSS feed from publisher
- `google_news`: Google News RSS for the domain
- `scraper`: HTML scraping fallback

**Priority Levels**:
- Priority 1: Official RSS feeds (fastest, most reliable)
- Priority 2: Google News RSS (quality-checked)
- Priority 3: HTML Scraper (slowest, most comprehensive)


## üîß MCP Tools Reference

### `get_articles`

Retrieve articles from a specific domain using multi-feed parallel fetching and 3-priority fallback.

**Parameters:**
| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `domain` | string | ‚úÖ | - | Domain name (supports partial matching: 'openai' ‚Üí 'openai.com') |
| `topic` | string | ‚ùå | null | Filter by keyword in title/summary/tags |
| `location` | string | ‚ùå | null | Filter by location keyword |
| `lastNDays` | integer | ‚ùå | 15 | Date range filter (1-365 days, capped at 15 by default) |
| `count` | integer | ‚ùå | 10 | Number of articles to return (max 100) |

**Example Request:**
```json
{
  "domain": "techcrunch",
  "topic": "AI",
  "location": "India",
  "lastNDays": 7,
  "count": 15
}
```

**Example Response:**
```json
{
  "sourceUsed": "priority_1 [official_rss, official_rss, official_rss]",
  "articles": [
    {
      "title": "OpenAI Launches GPT-5 in India",
      "url": "https://techcrunch.com/2024/12/08/openai-gpt5-india",
      "published_at": "2024-12-08T10:00:00+00:00",
      "summary": "OpenAI announced the launch of GPT-5 with special features for Indian languages...",
      "author": "John Doe",
      "tags": ["AI", "OpenAI", "India", "GPT-5"],
      "source_domain": "techcrunch.com"
    }
  ],
  "cached": false,
  "durationMs": 1245.5
}
```

**Behavior**:
- Default 15-day cap for "recent" news
- Parallel multi-feed fetching (if site has multiple RSS feeds)
- 3-priority fallback: RSS ‚Üí Google News (quality-checked) ‚Üí Scraper
- Threshold-based: ‚â•5 articles needed to skip next priority
- Deduplication by URL and title
- Sorted newest first

---

### `get_top_news`

Get top news from priority sites (6 configured, expandable to 12).

**Parameters:**
| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `count` | integer | ‚ùå | 10 | Number of articles to return (max 100) |
| `topic` | string | ‚ùå | null | Filter by keyword in title/summary/tags |
| `location` | string | ‚ùå | null | Filter by location keyword |
| `lastNDays` | integer | ‚ùå | 15 | Date range filter (1-365 days, capped at 15 by default) |

**Example Request:**
```json
{
  "count": 20,
  "topic": "technology",
  "location": "India",
  "lastNDays": 3
}
```

**Example Response:**
```json
{
  "sources_used": [
    "ndtv.com (priority_1)",
    "indianexpress.com (priority_1)",
    "timesofindia.indiatimes.com (priority_1)",
    "hindustantimes.com (priority_1)",
    "gadgets360.com (priority_1)",
    "economictimes.indiatimes.com (priority_1)"
  ],
  "articles": [
    {
      "title": "India's Tech Boom Continues",
      "url": "https://ndtv.com/india-news/tech-boom-2024-12-08",
      "published_at": "2024-12-08T09:00:00+00:00",
      "summary": "India's technology sector saw record growth...",
      "source_domain": "ndtv.com",
      "tags": ["technology", "India", "economy"]
    }
  ],
  "cached": false,
  "total_articles": 20,
  "durationMs": 2340.8
}
```

**Behavior**:
- Fetches from 6 priority sites in parallel
- Combines results from all sites
- Deduplicates by URL and title
- Filters by topic, location, date
- Sorts newest first
- Returns top N articles

---

### `health_check`

Check server health and configuration status.

**Parameters:** None

**Example Response:**
```json
{
  "status": "healthy",
  "version": "2.0.0",
  "configured_domains": 45,
  "priority_sites": 6,
  "explicit_sites": 39,
  "multi_feed_sites": 7,
  "total_rss_feeds": 26,
  "cache": {
    "size": 12,
    "ttl_seconds": 300,
    "max_size": 1000
  },
  "constants": {
    "MAX_RECENT_DAYS": 15,
    "DEFAULT_ARTICLE_COUNT": 10,
    "MIN_ARTICLES_THRESHOLD": 5
  },
  "timestamp": "2024-12-08T10:00:00+00:00"
}
```

---

### `get_metrics`

Get detailed server metrics for monitoring and observability.

**Parameters:** None

**Example Response:**
```json
{
  "metrics": {
    "uptime_seconds": 3600,
    "counters": {
      "get_articles_requests": 150,
      "get_articles_success": 145,
      "get_articles_domain_not_found": 3,
      "get_top_news_requests": 50,
      "cache_hits": 60,
      "cache_misses": 90,
      "fetch_success": 200,
      "fetch_error": 5
    },
    "histograms": {
      "get_articles_duration_ms": {
        "count": 150,
        "min": 120,
        "max": 2500,
        "avg": 1350,
        "p50": 1200,
        "p95": 2000,
        "p99": 2400
      },
      "get_top_news_duration_ms": {
        "count": 50,
        "min": 1800,
        "max": 4500,
        "avg": 2300,
        "p50": 2200,
        "p95": 3500,
        "p99": 4200
      }
    }
  },
  "timestamp": "2024-12-08T10:00:00+00:00"
}
```


## üß™ Testing & Verification

### Test Files

**Production Tests:**
- `verify_production.py` - Production configuration verification
- `test_all_domains.py` - Test all 45 configured domains
- `test_multifeed.py` - Multi-feed parallel fetching tests

**Run Tests:**
```bash
# Verify production configuration
python verify_production.py

# Test all domains
python test_all_domains.py

# Test multi-feed architecture
python test_multifeed.py
```

### Production Verification Results

‚úÖ **Configuration Loading**: 45 domains, 6 priority, 39 explicit  
‚úÖ **get_top_news()**: 5 articles from 6 priority sites  
‚úÖ **Flexible Matching**: All partial domain tests passed  
‚úÖ **Multi-Feed**: 26 articles from 7 sites, 0 duplicates, 7.7s total  
‚úÖ **All Requirements**: 9/9 verified and implemented

## üìÅ Project Structure

```
NewsNexus/
‚îú‚îÄ‚îÄ main.py                          # MCP server (production-ready)
‚îú‚îÄ‚îÄ sites.json                       # 45 domains configuration
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ REQUIREMENTS_VERIFICATION.md     # Detailed requirements report
‚îú‚îÄ‚îÄ MIGRATION_COMPLETE.md            # Migration documentation
‚îú‚îÄ‚îÄ FILTERING_GUIDE.md               # Comprehensive filtering guide
‚îú‚îÄ‚îÄ verify_production.py             # Production verification script
‚îú‚îÄ‚îÄ test_all_domains.py              # Test all domains
‚îú‚îÄ‚îÄ test_multifeed.py                # Multi-feed tests
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py                 # Unit tests
‚îî‚îÄ‚îÄ __pycache__/
```

## üìä Performance Metrics

### Typical Response Times

| Operation | Avg Time | P95 Time | Notes |
|-----------|----------|----------|-------|
| Single domain (RSS hit) | 1.2s | 2.0s | Official RSS feed |
| Single domain (Google News) | 1.8s | 2.5s | With quality check |
| Single domain (Scraper) | 2.5s | 3.5s | Deep HTML extraction |
| Multi-feed (6 feeds) | 1.3s | 2.2s | Parallel fetching |
| get_top_news() (6 sites) | 2.3s | 3.5s | Parallel multi-site |

### Multi-Feed Performance

- **7 sites** with 26 total RSS feeds
- **Parallel workers**: 8 max concurrent threads
- **Average**: ~1.3s per multi-feed site
- **Deduplication**: 0 duplicates across feeds
- **Example**: NDTV (5 feeds) ‚Üí 10 unique articles in 1.5s

## üîí Security Features

1. **Input Validation**
   - Domain format validation (regex, length checks)
   - URL security (blocks file://, javascript:, private IPs)
   - Parameter sanitization (XSS prevention)
   - Length limits on all inputs (max 500 chars)

2. **Rate Limiting**
   - Per-domain request limits (10 req/60s default)
   - Sliding window algorithm
   - Configurable via environment variables

3. **Error Handling**
   - Specific exception types (no broad catches)
   - Proper JSON-RPC error codes
   - No sensitive info in error messages
   - Structured logging with metrics

4. **Content Filtering**
   - URL normalization & deduplication
   - Title fuzzy matching
   - Date validation (no future dates)
   - Source quality checks

## üöÄ Production Deployment

### Requirements

- Python 3.13+
- 512 MB RAM minimum (1 GB recommended)
- 100 MB disk space
- Internet connection (for fetching articles)

### Environment Setup

```bash
# Set environment variables (optional)
export NEWSNEXUS_LOG_LEVEL=INFO
export NEWSNEXUS_CACHE_TTL=300
export NEWSNEXUS_MAX_ARTICLES=50

# Run MCP server
python main.py
```

### MCP Integration

Add to your MCP configuration file:

**Claude Desktop** (`%APPDATA%\Claude\config.json`):
```json
{
  "mcpServers": {
    "news-nexus": {
      "command": "python",
      "args": ["C:\\Swdtools\\NewsNexus\\main.py"]
    }
  }
}
```

**GitHub Copilot / VS Code** (`.vscode/mcp_settings.json`):
```json
{
  "mcpServers": {
    "news-nexus": {
      "command": "python",
      "args": ["C:\\Swdtools\\NewsNexus\\main.py"]
    }
  }
}
```

### Monitoring & Observability

```python
# Check health
health_check()

# Get metrics
metrics = get_metrics()
print(f"Uptime: {metrics['uptime_seconds']}s")
print(f"Success rate: {metrics['counters']['get_articles_success'] / metrics['counters']['get_articles_requests'] * 100}%")
print(f"Cache hit rate: {metrics['counters']['cache_hits'] / (metrics['counters']['cache_hits'] + metrics['counters']['cache_misses']) * 100}%")
```

## üí° Best Practices

### For Users

1. **Use partial domain names** for convenience: `openai` instead of `openai.com`
2. **Specify topic and location** for better filtering
3. **Use lastNDays** parameter for specific date ranges
4. **Start with get_top_news()** for general news browsing
5. **Check health_check()** if experiencing issues

### For Developers

1. **Add new domains to sites.json** with proper priority levels
2. **Test with verify_production.py** after configuration changes
3. **Monitor metrics** using get_metrics() for performance insights
4. **Use structured logging** for debugging (set LOG_LEVEL=DEBUG)
5. **Configure caching** appropriately for your use case

## üÜï Recent Updates (v2.0)

- ‚úÖ **Priority-grouped parallel fetching** (faster multi-feed)
- ‚úÖ **Flexible domain matching** (partial names supported)
- ‚úÖ **Google News quality controls** (redirect URL filtering)
- ‚úÖ **15-day default cap** for recent news
- ‚úÖ **Threshold-based fallback** (‚â•5 articles minimum)
- ‚úÖ **8-layer filtering engine** (comprehensive)
- ‚úÖ **6 priority sites** for top news (expandable to 12)
- ‚úÖ **26 RSS feeds** across 7 multi-feed sites
- ‚úÖ **Reverse chronological** sorting (newest first)

## üìù License

MIT License - see LICENSE for details

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

**Built for the MCP ecosystem** | **Production-ready** | **Fast & Reliable**

For detailed technical documentation, see:
- `REQUIREMENTS_VERIFICATION.md` - Full requirements verification
- `FILTERING_GUIDE.md` - Comprehensive filtering guide
- `MIGRATION_COMPLETE.md` - Migration documentation
